const char* const templates_GB_jit_AxB_dot3_phase3_vsvs_cu = "templates/GB_jit_AxB_dot3_phase3_vsvs.cu\n"
"//******************************************************************************\n"
"//  Sparse dot version of Matrix-Matrix multiply with mask \n"
"//  Each thread in this kernel is responsible for m vector-pairs(x,y), \n"
"//  finding intersections and producting the final dot product for each\n"
"//  using a serial merge algorithm on the sparse vectors. \n"
"//  m = 256/sz, where sz is in {4, 16, 64, 256}\n"
"//  For a vector-pair, sz = xnz + ynz \n"
"//  Template on <T_C, T_M, T_A, T_B>\n"
"//  Parameters:\n"
"\n"
"//  int64_t start          <- start of vector pairs for this kernel\n"
"//  int64_t end            <- end of vector pairs for this kernel\n"
"//  int64_t *Bucket        <- array of pair indices for all kernels \n"
"//  matrix<T_C> *C         <- result matrix \n"
"//  matrix<T_M> *M         <- mask matrix\n"
"//  matrix<T_A> *A         <- input matrix A\n"
"//  matrix<T_B> *B         <- input matrix B\n"
"//  int sz                 <- nnz of very sparse vectors\n"
"\n"
"//  Blocksize is 1024, uses warp and block reductions to count zombies produced.\n"
"//******************************************************************************\n"
"#include <limits>\n"
"#include <cstdint>\n"
"#include <stdio.h>\n"
"#include <cooperative_groups.h>\n"
"//#include \"GB_matrix.h\"\n"
"#include \"matrix.h\"\n"
"#include \"mySemiRing.h\"\n"
"\n"
"using namespace cooperative_groups;\n"
"\n"
"template< typename T, int tile_sz>\n"
"__inline__ __device__ \n"
"T warp_ReduceSumPlus( thread_block_tile<tile_sz> g, T val)\n"
"{\n"
"    // Each iteration halves the number of active threads\n"
"    // Each thread adds its partial sum[i] to sum[lane+i]\n"
"    for (int i = g.size() / 2; i > 0; i /= 2) {\n"
"        //printf(\"thd%d   %d OP %d is %d\\n\", threadIdx.x, val, fold, OP( val, fold));\n"
"        val +=  g.shfl_down( val, i);\n"
"    }\n"
"    return val; // note: only thread 0 will return full sum\n"
"}\n"
"\n"
"template< typename T, int tile_sz>\n"
"__inline__ __device__ \n"
"T warp_Reduce( thread_block_tile<tile_sz> g, T val)\n"
"{\n"
"    // Each iteration halves the number of active threads\n"
"    // Each thread adds its partial sum[i] to sum[lane+i]\n"
"    for (int i = g.size() / 2; i > 0; i /= 2) {\n"
"        //printf(\"thd%d   %d OP %d is %d\\n\", threadIdx.x, val, fold, OP( val, fold));\n"
"        T next = g.shfl_down( val, i) ;\n"
"        val = GB_ADD( sum, next ) ; \n"
"    }\n"
"    //if (threadIdx.x ==0) printf(\"thd%d single warp sum is %d\\n\", threadIdx.x,  val);\n"
"    return val; // note: only thread 0 will return full sum\n"
"}\n"
"\n"
"template<typename T, int warpSize>\n"
"__inline__ __device__\n"
"T block_ReduceSum(thread_block g, T val)\n"
"{\n"
"  static __shared__ T shared[warpSize]; // Shared mem for 32 partial sums\n"
"  int lane = threadIdx.x % warpSize;\n"
"  int wid = threadIdx.x / warpSize;\n"
"  thread_block_tile<warpSize> tile = tiled_partition<warpSize>( g );\n"
"\n"
"  // Each warp performs partial reduction\n"
"  val = warp_ReduceSumPlus<T, warpSize>( tile, val);    \n"
"\n"
"  // Wait for all partial reductions\n"
"  if (lane==0) { \n"
"     //printf(\"thd%d warp%d sum is %d\\n\", threadIdx.x, wid, val);\n"
"     shared[wid]=val; // Write reduced value to shared memory\n"
"     //printf(\"thd%d stored warp %d sum %d\\n\", threadIdx.x, wid, val);\n"
"  }\n"
"  tile.sync();              // Wait for all partial reductions\n"
"\n"
"  if (wid > 0 || gridDim.x == 1 ) return val;\n"
"  //read from shared memory only if that warp existed\n"
"  val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : 0;\n"
"  //printf(\"thd%d warp loaded val = %d\\n\", threadIdx.x, lane, val);\n"
"\n"
"  \n"
"  if (wid==0) val = warp_ReduceSumPlus<T, warpSize>( tile, val); //Final reduce within first warp\n"
"\n"
"  return val;\n"
"}\n"
"\n"
"template< typename T_C, typename T_A, typename T_B, typename T_X, typename T_Y, typename T_Z>\n"
"__global__ void AxB_dot3_phase3_vsvs\n"
"( \n"
"  int64_t start, \n"
"  int64_t end,\n"
"  int64_t *Bucket, \n"
"  GrB_Matrix C, \n"
"  GrB_Matrix M, \n"
"  GrB_Matrix A, \n"
"  GrB_Matrix B,\n"
"  int sz \n"
")\n"
"{\n"
"   // sz = expected non-zeros per dot \n"
"   int m = 256/sz;\n"
"   int nvecs = end - start;\n"
"   int dpt = nvecs/32;\n"
"   m = dpt < m ? dpt : m;\n"
"   //__shared__ int zombie_local[32];\n"
"   /*\n"
"   if( threadIdx.x ==0 && blockIdx.x == 0)\n"
"      printf(\" %d dots/thrd, nvecs = %d blockDim=%d\\n\",sz, nvecs, blockDim.x);\n"
"   __syncthreads();\n"
"   */\n"
"   int dots = (nvecs +m -1)/m; \n"
"   int zc = 0;\n"
"\n"
"   T_A *Ax = (T_A *)A->x  ;\n"
"   T_B *Bx = (T_B *)B->x  ;\n"
"   T_C *Cx = (T_C *)C->x  ;\n"
"   int64_t *Ci = C->i ;\n"
"   int64_t *Mi = M->i ;\n"
"   int64_t *Ai = A->i ;\n"
"   int64_t *Bi = B->i ;\n"
"   int64_t *Ap = A->p ;\n"
"   int64_t *Bp = B->p ;\n"
"     \n"
"   for ( int tid= threadIdx.x +blockDim.x*blockIdx.x;\n"
"             tid < dots;\n"
"             tid += blockDim.x * gridDim.x) {\n"
"      int pair_id, im; \n"
"       //if (threadIdx.x ==0)\n"
"       //  printf(\"thd%u pi=%lld\\n\",tid, start+threadIdx.x); \n"
"       //  __syncthreads();\n"
"\n"
"      for (pair_id = start+tid, im = 0; \n"
"           im < m && pair_id < end;  \n"
"           ++im,     pair_id += dots ){\n"
"\n"
"         int64_t i = Mi [pair_id] ;\n"
"         int64_t j = Ci [pair_id]>>4 ; \n"
"         //int64_t i = M->i[pair_id];\n"
"         //int64_t j = C->i[pair_id] >> 4;\n"
"      //if (threadIdx.x ==0)\n"
"      //   printf(\"thd%u i,j=%lld,%lld\\n\",tid, i,j); \n"
"      //   __syncthreads();\n"
"         \n"
"     //  printf(\"thd%d pi=%d xn=%lld yn=%lld\\n\",tid, pair_id, \n"
"     //                 A->p[i+1]- A->p[i],\n"
"     //                 B->p[j+1]- B->p[j]);\n"
"\n"
"         int64_t pA       = Ap[i];\n"
"         int64_t pA_end   = Ap[i+1];\n"
"         int64_t pB       = Bp[j]; \n"
"         int64_t pB_end   = Bp[j+1]; \n"
"\n"
"         T_A aki;\n"
"         T_B bkj;\n"
"         T_Z cij ;\n"
"\n"
"         bool cij_exists = false;\n"
"\n"
"         while (pA < pA_end && pB < pB_end)\n"
"         {\n"
"            int64_t ia = Ai [pA] ;\n"
"            int64_t ib = Bi [pB] ;\n"
"            if (ia < ib)\n"
"            { \n"
"                // A(ia,i) appears before B(ib,j)\n"
"                pA++ ;\n"
"            }\n"
"            else if (ib < ia)\n"
"            { \n"
"                // B(ib,j) appears before A(ia,i)\n"
"                pB++ ;\n"
"            }\n"
"            else // ia == ib == k\n"
"            { \n"
"                // A(k,i) and B(k,j) are the next entries to merge\n"
"                #if defined ( GB_PHASE_1_OF_2 )\n"
"                cij_exists = true ;\n"
"                break ;\n"
"                #else\n"
"                GB_DOT_MERGE ;\n"
"                //GB_DOT_TERMINAL (cij) ;         // break if cij == terminal\n"
"                pA++ ;\n"
"                pB++ ;\n"
"                #endif\n"
"            }\n"
"         }\n"
"         if (cij_exists){\n"
"            GB_PUTC ( Ci[pair_id] = i ) ;\n"
"            GB_PUTC ( Cx[pair_id] = (T_C)cij ) ;\n"
"         }\n"
"         else{\n"
"            zc++; \n"
"            //printf(\" %lld, %lld is zombie %d!\\n\",i,j,zc);\n"
"            GB_PUTC( Ci[pair_id] = GB_FLIP( i ) ) ;\n"
"         }\n"
"      }\n"
"  \n"
"   }\n"
"   //printf(\"thd%d zombie count = %d\\n\",threadIdx.x,zc);\n"
"   zc = block_ReduceSum<int , 32>( this_thread_block(), zc); \n"
"   \n"
"   __syncthreads();\n"
"   if( threadIdx.x == 0 && zc > 0) {\n"
"      //printf(\"block zombie count = %d\\n\",zc);\n"
"      atomicAdd( (unsigned long long int*)&(C->zombie_count), (unsigned long long int)zc);\n"
"      //C->zombie_count += (unsigned long long int)zc;\n"
"      //printf(\"blk:%d Czombie = %lld\\n\", blockIdx.x,C->zombie_count);\n"
"   }\n"
"   \n"
"}\n"
;
