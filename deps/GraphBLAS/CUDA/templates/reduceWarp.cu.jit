const char* const templates_reduceWarp_cu = "templates/reduceWarp.cu\n"
"//------------------------------------------------------------------------------\n"
"// reduceUnrolled.cu\n"
"//------------------------------------------------------------------------------\n"
"\n"
"// The reduceUnrolled CUDA kernel reduces an array g_idata of size n, of any\n"
"// type T, to an array g_odata of size grid.x.  Each threadblock (blockIdx.x)\n"
"// reduces its portion of g_idata to a single scalar, g_odata [blockIdx.x].\n"
"\n"
"// Both the grid and block are 1D, so blockDim.x is the # threads in a\n"
"// threadblock, and the # of threadblocks is grid.x\n"
"\n"
"// Let b = blockIdx.x, and let s be blockDim.x.\n"
"// Each threadblock owns s*8 contiguous items in the input data.\n"
"\n"
"// Thus, threadblock b owns g_idata [b*s*8 ... min(n,(b+1)*s*8-1)].  It's job\n"
"// is to reduce this data to a scalar, and write it to g_odata [b].\n"
"\n"
"#include <cooperative_groups.h>\n"
"\n"
"using namespace cooperative_groups;\n"
"\n"
"template< typename T, int tile_sz>\n"
"__inline__ __device__ \n"
"T warp_ReduceSum( thread_block_tile<tile_sz> g, T val)\n"
"{\n"
"    // Each iteration halves the number of active threads\n"
"    // Each thread adds its partial sum[i] to sum[lane+i]\n"
"    for (int i = g.size() / 2; i > 0; i /= 2) {\n"
"        T fold = g.shfl_down( val, i);\n"
"        //printf(\"thd%d   %d OP %d is %d\\n\", threadIdx.x, val, fold, OP( val, fold));\n"
"        val = OP( val, fold );\n"
"    }\n"
"    //if (threadIdx.x ==0) printf(\"thd%d single warp sum is %d\\n\", threadIdx.x,  val);\n"
"    return val; // note: only thread 0 will return full sum\n"
"}\n"
"\n"
"template<typename T, int warpSize>\n"
"__inline__ __device__\n"
"T block_ReduceSum(thread_block g, T val)\n"
"{\n"
"  static __shared__ T shared[warpSize]; // Shared mem for 32 partial sums\n"
"  int lane = threadIdx.x % warpSize;\n"
"  int wid = threadIdx.x / warpSize;\n"
"  thread_block_tile<warpSize> tile = tiled_partition<warpSize>( g );\n"
"\n"
"  // Each warp performs partial reduction\n"
"  val = warp_ReduceSum<T, warpSize>( tile, val);    \n"
"\n"
"  // Wait for all partial reductions\n"
"  if (lane==0) { \n"
"     //printf(\"thd%d warp%d sum is %d\\n\", threadIdx.x, wid, val);\n"
"     shared[wid]=val; // Write reduced value to shared memory\n"
"     //printf(\"thd%d stored warp %d sum %d\\n\", threadIdx.x, wid, val);\n"
"  }\n"
"  __syncthreads();              // Wait for all partial reductions\n"
"\n"
"  if (wid > 0 || gridDim.x == 1 ) return val;\n"
"  //read from shared memory only if that warp existed\n"
"  val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] : MONOID_IDENTITY;\n"
"  //printf(\"thd%d warp loaded val = %d\\n\", threadIdx.x, lane, val);\n"
"\n"
"  \n"
"  if (wid==0) val = warp_ReduceSum<T, warpSize>( tile, val); //Final reduce within first warp\n"
"\n"
"  return val;\n"
"}\n"
"\n"
"template< typename T>\n"
"__global__ void reduceWarp\n"
"(\n"
"    T *g_idata,     // array of size n\n"
"    T *g_odata,     // array of size grid.x\n"
"    unsigned int N\n"
")\n"
"{\n"
"    // set thread ID\n"
"    unsigned int tid = threadIdx.x ;\n"
"\n"
"    // each thread tid reduces its result into sum\n"
"    T sum = (T) MONOID_IDENTITY;\n"
"\n"
"    for(int i = blockIdx.x * blockDim.x + threadIdx.x; \n"
"        i < N; \n"
"        i += blockDim.x * gridDim.x) {\n"
"        sum = OP( sum, g_idata[i]);\n"
"    }\n"
"    //printf(\"thd%d  sum is %d\\n\", threadIdx.x + blockDim.x*blockIdx.x, sum);\n"
"    __syncthreads();\n"
"    //--------------------------------------------------------------------------\n"
"    // reduce work [0..s-1] to a single scalar\n"
"    //--------------------------------------------------------------------------\n"
"    // this assumes blockDim is a multiple of 32\n"
"    sum = block_ReduceSum<T , 32>( this_thread_block(), sum); \n"
"\n"
"    // write result for this block to global mem\n"
"    if (tid == 0)\n"
"    {\n"
"        // printf (\"final %d : %g\\n\", b, (double) work [0]) ;\n"
"        g_odata [blockIdx.x] = sum ;\n"
"    }\n"
"}\n"
"\n"
;
