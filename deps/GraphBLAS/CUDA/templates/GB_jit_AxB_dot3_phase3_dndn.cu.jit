const char* const templates_GB_jit_AxB_dot3_phase3_dndn_cu = "templates/GB_jit_AxB_dot3_phase3_dndn.cu\n"
"\n"
"//------------------------------------------------------------------------------\n"
"// AxB_dot3_phase3_dndn.cu \n"
"//------------------------------------------------------------------------------\n"
"\n"
"// This CUDA kernel produces the semi-ring product of two\n"
"// sparse matrices of types T_A and T_B and common index space size n, to a  \n"
"// output matrix of type T_C. The matrices are sparse, with different numbers\n"
"// of non-zeros and different sparsity patterns. \n"
"// ie. we want to produce C = A'*B in the sense of the given semi-ring.\n"
"\n"
"// This version uses a simple warp-based dense dot product algorithm, when the\n"
"// vectors coming from both A and B are dense, for any size of N.\n"
"\n"
"// Both the grid and block are 1D, so blockDim.x is the # threads in a\n"
"// threadblock, and the # of threadblocks is grid.x\n"
"\n"
"// Let b = blockIdx.x, and let s be blockDim.x. s= 32 with a variable number\n"
"// of active threads = min( min(nzA, nzB), 32) \n"
"\n"
"// Thus, threadblock b owns a semi-ring dot product on a pair of vectors. \n"
"// The work is to load the data, do the multiply and add work and finally \n"
"// reduce this data to a scalar, and write it to Cx[pair].\n"
"\n"
"//  int64_t start          <- start of vector pairs for this kernel\n"
"//  int64_t end            <- end of vector pairs for this kernel\n"
"//  int64_t *Bucket        <- array of pair indices for all kernels \n"
"//  GrB_Matrix C           <- result matrix \n"
"//  GrB_Matrix M           <- mask matrix\n"
"//  GrB_Matrix A           <- input matrix A\n"
"//  GrB_Matrix B           <- input matrix B\n"
"//  int sz                 <- size parameter (not used) \n"
"\n"
"#include <limits>\n"
"#include <cstdint>\n"
"#include <cooperative_groups.h>\n"
"#include \"matrix.h\"\n"
"#include \"mySemiRing.h\"\n"
"\n"
"// Using tile size fixed at compile time, we don't need shared memory\n"
"#define tile_sz 32 \n"
"\n"
"using namespace cooperative_groups;\n"
"\n"
"template< typename T, int warp_sz>\n"
"__inline__ __device__ T warp_ReduceSum(thread_block_tile<warp_sz> g, T val)\n"
"{\n"
"    // Each iteration halves the number of active threads\n"
"    // Each thread adds its partial sum[i] to sum[lane+i]\n"
"    for (int i = g.size() / 2; i > 0; i /= 2)\n"
"    {\n"
"        T next = g.shfl_down( val, i) ;\n"
"        val = GB_ADD( val, next ); \n"
"    }\n"
"    return val; // note: only thread 0 will return full sum\n"
"}\n"
"\n"
"template<typename T, int warpSize >\n"
"__inline__ __device__\n"
"T block_ReduceSum(thread_block g, T val, T Ident)\n"
"{\n"
"  static __shared__ T shared[warpSize]; // Shared mem for 32 partial sums\n"
"  int lane = threadIdx.x % warpSize;\n"
"  int wid = threadIdx.x / warpSize;\n"
"  thread_block_tile<warpSize> tile = tiled_partition<warpSize>(g);\n"
"\n"
"  // Each warp performs partial reduction\n"
"  val = warp_ReduceSum< T, warpSize>(tile, val);    \n"
"\n"
"  if (lane==0) shared[wid] = val; // Write reduced value to shared memory\n"
"\n"
"  //tile.sync();                    // Wait for all partial reductions\n"
"\n"
"  if (wid > 0 || gridDim.x == 1 ) return val;\n"
"\n"
"  //read from shared memory only if that warp existed\n"
"  val = (threadIdx.x < blockDim.x / warpSize) ? shared[lane] :  Ident  ;\n"
"\n"
"  if (wid==0) val = warp_ReduceSum< T, warpSize>(tile,val); //Final reduce within first warp\n"
"\n"
"  return val;\n"
"}\n"
"\n"
"\n"
"template< typename T_C, typename T_A, typename T_B, typename T_X, typename T_Y, typename T_Z>\n"
"__global__ void AxB_dot3_phase3_dndn \n"
"(\n"
"    int64_t start,\n"
"    int64_t end,\n"
"    int64_t *Bucket,\n"
"    GrB_Matrix C,\n"
"    GrB_Matrix M,\n"
"    GrB_Matrix A,\n"
"    GrB_Matrix B,\n"
"    int sz\n"
")\n"
"{\n"
"\n"
"    T_A *Ax = (T_A*)A->x;\n"
"    T_B *Bx = (T_B*)B->x;\n"
"    T_C *Cx = (T_C*)C->x;\n"
"    int64_t *Mi = M->i;\n"
"    int64_t *Ci = C->i;\n"
"    int64_t *Ap = A->p;\n"
"    int64_t *Bp = B->p;\n"
"\n"
"    // zombie count\n"
"    int zc = 0;\n"
"    int64_t pair_id;\n"
"\n"
"    // total items to be inspected\n"
"    int64_t nnzA = 0;\n"
"    int64_t nnzB = 0;\n"
"    int s = blockDim.x;\n"
"\n"
"    // Main loop over pairs \n"
"    for (pair_id = start + blockIdx.x; //warp per pair \n"
"         pair_id < end;  \n"
"         pair_id += gridDim.x ){\n"
"\n"
"         int64_t i = Mi[pair_id];\n"
"         int64_t j = Ci[pair_id] >> 4;\n"
"\n"
"         int64_t pA = Ap[i];\n"
"         int64_t xend   = Ap[i+1];\n"
"         nnzA = xend - pA;\n"
"\n"
"         int64_t pB = Bp[j]; \n"
"         int64_t yend   = Bp[j+1]; \n"
"         nnzB = yend - pB;\n"
"\n"
"    /*\n"
"    if (threadIdx.x == 0 ){\n"
"        printf(\" i,j = %d,%d  nnz= %d xstart,end = %d,%d  ystart,end = %d,%d\\n\",\n"
"            (int)i,(int)j,  (int)nnzA, (int)xstart,(int)xend, (int)ystart, (int)yend);\n"
"    }\n"
"    __syncthreads();                                          \n"
"    */\n"
"\n"
"    \n"
"    // convert global data pointer to the local pointer of this block\n"
"    T_A  aki; // *xdata = &Ax[xstart]; \n"
"    T_B  bkj; // *ydata = &Bx[ystart];\n"
"    T_Z  cij;\n"
"\n"
"    GB_GETA ( aki=(T_Z)Ax[pA+threadIdx.x] ) ;             // aki = A(0,i)\n"
"    GB_GETB ( bkj=(T_Z)Bx[pB+threadIdx.x] ) ;             // bkj = B(0,j)\n"
"    GB_C_MULT ( cij, aki, bkj ) ;                        // cij = aki * bkj\n"
"\n"
"    for ( int tid = threadIdx.x + s; tid < nnzA; tid+= s) { \n"
"          // cij += A(k,i) * B(k,j)\n"
"          // GB_DOT_TERMINAL ( cij ) ;             // break if cij == terminal\n"
"          GB_GETA ( aki=(T_Z)Ax[pA+tid] ) ;         // aki = A(k,i)\n"
"          GB_GETB ( bkj=(T_Z)Bx[pB+tid] ) ;        // bkj = B(k,j)\n"
"          GB_MULTADD ( cij, aki, bkj ) ;        // cij += aki * bkj\n"
"    }\n"
"\n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // reduce per-thread sums to a single scalar\n"
"    //--------------------------------------------------------------------------\n"
"    thread_block_tile<32> tile = tiled_partition<32>( this_thread_block() );\n"
"    cij = warp_ReduceSum<T_Z, 32> ( tile, cij);\n"
"\n"
"    // write result for this block to global mem\n"
"    if (threadIdx.x == 0)\n"
"    {\n"
"       //printf(\"tid: %d final sum after reduce = %d\\n\", threadIdx.x, sum);\n"
"       GB_PUTC( Cx[pair_id]=(T_C)cij ) ;\n"
"       GB_PUTC( Ci[pair_id]=i ) ;\n"
"    }\n"
"    //__syncthreads ( ) ;\n"
"  }\n"
"\n"
"}\n"
"\n"
;
