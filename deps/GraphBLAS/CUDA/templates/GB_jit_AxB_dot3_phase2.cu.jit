const char* const templates_GB_jit_AxB_dot3_phase2_cu = "templates/GB_jit_AxB_dot3_phase2.cu\n"
"//------------------------------------------------------------------------------\n"
"// templates/GB_AxB_cuda_dot3_phase2: fill the global buckets\n"
"//------------------------------------------------------------------------------\n"
"\n"
"// TODO describe me\n"
"\n"
"#define GB_KERNEL\n"
"#include <cstdint>\n"
"#include \"GB_cuda_buckets.h\"\n"
"#include \"matrix.h\"\n"
"#include <cooperative_groups.h>\n"
"#include \"local_cub/block/block_scan.cuh\"\n"
"\n"
"using namespace cooperative_groups;\n"
"\n"
"// A stateful callback functor that maintains a running prefix to be applied\n"
"// during consecutive scan operations.\n"
"struct BlockPrefixCallbackOp\n"
"{\n"
"   // Running prefix\n"
"   int64_t running_total;\n"
"   // Constructor\n"
"   __device__ BlockPrefixCallbackOp(int64_t running_total) : running_total(running_total) {}\n"
"\n"
"   // Callback operator to be entered by the first warp of threads in the block.\n"
"   // Thread-0 is responsible for returning a value for seeding the block-wide scan.\n"
"   __device__ int64_t operator()(int64_t block_aggregate)\n"
"   {\n"
"     int64_t old_prefix = running_total;\n"
"     running_total += block_aggregate;\n"
"     return old_prefix;\n"
"   }\n"
"};\n"
"\n"
"__inline__ \n"
"__device__ void blockBucketExclusiveSum(int bucketId, int64_t *d_data, int nblocks)\n"
"{\n"
"   #define blocksize  32\n"
"\n"
"   // Specialize BlockScan for a 1D block of 32 threads\n"
"   typedef cub::BlockScan<int64_t, 32, cub::BLOCK_SCAN_WARP_SCANS> BlockScan; \n"
"\n"
"   // Allocate shared memory for BlockScan\n"
"   __shared__ typename BlockScan::TempStorage temp_storage;\n"
"\n"
"   // Initialize running total\n"
"   BlockPrefixCallbackOp prefix_op(0);\n"
"\n"
"   // Have the block iterate over segments of items\n"
"   int64_t data=0;\n"
"\n"
"   int64_t *blockbucket= d_data;\n"
"\n"
"   for (int block_id = 0; block_id < nblocks; block_id += blocksize)\n"
"   {\n"
"    // Load a segment of consecutive items that are blocked across threads\n"
"\n"
"    //printf(\"block %d entering sum\\n\",blockIdx.x);\n"
"      int loc = block_id + threadIdx.x;\n"
"      if ( loc < nblocks)\n"
"      { \n"
"        //printf(\"block %di loading tid=%d\\n\",block_id,tid);\n"
"        data  = blockbucket[bucketId*nblocks    +loc ] ; \n"
"      }\n"
"      __syncthreads();\n"
"\n"
"      //printf(\"bb%d_%d s0 before prefix= %ld \\n\", block_id,bucketId, \n"
"      //                     blockbucket[bucketId*nblocks + block_id+threadIdx.x] )  ; \n"
"      // Collectively compute the block-wide exclusive prefix sum\n"
"      BlockScan(temp_storage).ExclusiveSum( data, data, prefix_op);\n"
"      __syncthreads();\n"
"\n"
"      if ( loc < nblocks)\n"
"      { \n"
"        blockbucket[bucketId*nblocks   +loc ]  = data  ; \n"
"      }\n"
"      __syncthreads();\n"
"\n"
"        //printf(\"bb%d_%d = %ld \\n\", block_id, bucketId, blockbucket[bucketId*nblocks+block_id+threadIdx.x] )  ; \n"
"      \n"
"      data = 0;\n"
"   }\n"
"}\n"
"\n"
"\n"
"template< typename T, int tile_sz>\n"
"__inline__ __device__ \n"
"T warp_ReduceSumPlus( thread_block_tile<tile_sz> tile, T val)\n"
"{\n"
"    // Each iteration halves the number of active threads\n"
"    // Each thread adds its partial sum[i] to sum[lane+i]\n"
"    for (int i = tile.size() / 2; i > 0; i /= 2) {\n"
"        val +=  tile.shfl_down( val, i);\n"
"    }\n"
"    return val; // note: only thread 0 will return full sum\n"
"}\n"
"\n"
"template<typename T, int warpSize>\n"
"__inline__ __device__\n"
"T block_ReduceSum(thread_block g, T val)\n"
"{\n"
"  static __shared__ T shared[warpSize]; // Shared mem for 32 partial sums\n"
"  int lane = threadIdx.x % warpSize;\n"
"  int wid = threadIdx.x / warpSize;\n"
"  thread_block_tile<warpSize> tile = tiled_partition<warpSize>( g );\n"
"\n"
"  // Each warp performs partial reduction\n"
"  val = warp_ReduceSumPlus<T, warpSize>( tile, val);    \n"
"\n"
"  // Wait for all partial reductions\n"
"  if (lane==0) { \n"
"     //printf(\"thd%d warp%d sum is %d\\n\", threadIdx.x, wid, val);\n"
"     shared[wid]=val; // Write reduced value to shared memory\n"
"     //printf(\"thd%d stored warp %d sum %d\\n\", threadIdx.x, wid, val);\n"
"  }\n"
"  __syncthreads();              // Wait for all partial reductions\n"
"\n"
"  if (wid > 0 ) return val ;\n"
"  //Final reduce within first warp\n"
"  if (wid==0) val = warp_ReduceSumPlus<T, warpSize>( tile, val) ; \n"
"\n"
"  return val;\n"
"}\n"
"\n"
"// GB_AxB_cuda_dot3_phase2 is a CUDA kernel that takes as input the\n"
"// nanobuckets and blockbucket arrays computed by the first phase kernel,\n"
"// GB_AxB_cuda_dot3_phase1.  The launch geometry of this kernel must match the\n"
"// GB_AxB_cuda_dot3_phase1 kernel, with the same # of threads and threadblocks.\n"
"\n"
"__global__ \n"
"void GB_AxB_dot3_phase2\n"
"(\n"
"    // input, not modified:\n"
"    int64_t *__restrict__ nanobuckets,    // array of size 12-blockDim.x-by-nblocks\n"
"    int64_t *__restrict__ blockbucket,    // global bucket count, of size 12*nblocks\n"
"    // output:\n"
"    int64_t *__restrict__ bucketp,        // global bucket cumsum, of size 13 \n"
"    int64_t *__restrict__ bucket,         // global buckets, of size cnz (== mnz)\n"
"    int64_t *__restrict__ offset,         // global offsets, for each bucket\n"
"    // inputs, not modified:\n"
"    GrB_Matrix C,             // output matrix\n"
"    const int64_t cnz,        // number of entries in C and M \n"
"    const int nblocks         // input number of blocks to reduce\n"
")\n"
"{\n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // get C and M\n"
"    //--------------------------------------------------------------------------\n"
"\n"
"    //int64_t *Ci = C->i ;       // for zombies, or bucket assignment\n"
"\n"
"    // Ci [p] for an entry C(i,j) contains either GB_FLIP(i) if C(i,j) is a\n"
"    // zombie, or (k << 4) + bucket otherwise, where C(:,j) is the kth vector\n"
"    // of C (j = Ch [k] if hypersparse or j = k if standard sparse), and\n"
"    // where bucket is the bucket assignment for C(i,j).  This phase does not\n"
"    // need k, just the bucket for each entry C(i,j).\n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // sum up the bucket counts of prior threadblocks\n"
"    //--------------------------------------------------------------------------\n"
"\n"
"    // blockbucket is an array of size 12-by-nblocks, held by row.  The\n"
"    // entry blockbucket [bucket * nblocks + t] holds the # of entries\n"
"    // in the bucket (in range 0 to 11) found by threadblock t.\n"
"\n"
"\n"
"    //__shared__ uint64_t offset [12] ;\n"
"    uint64_t s_0=0;\n"
"    uint64_t s_1=0;\n"
"    uint64_t s_2=0;\n"
"    uint64_t s_3=0;\n"
"    uint64_t s_4=0;\n"
"    uint64_t s_5=0;\n"
"    uint64_t s_6=0;\n"
"    uint64_t s_7=0;\n"
"    uint64_t s_8=0;\n"
"    uint64_t s_9=0;\n"
"    uint64_t s_10=0;\n"
"    uint64_t s_11=0;\n"
"\n"
"    thread_block_tile<32> tile = tiled_partition<32>(this_thread_block() );\n"
"\n"
"    //printf(\"block %d entering sum\\n\",blockIdx.x);\n"
"    int tid = threadIdx.x  + blockIdx.x*blockDim.x;\n"
"    #define reduceBucket( B )    \\\n"
"     for( tid = threadIdx.x + blockIdx.x*blockDim.x; \\\n"
"          tid < nblocks;  \\\n"
"          tid += blockDim.x*gridDim.x) \\\n"
"     {                           \\\n"
"        s_ ## B  += blockbucket[  B *nblocks +tid] ;  \\\n"
"     } \\\n"
"     __syncthreads(); \\\n"
"     s_ ## B  = warp_ReduceSumPlus<uint64_t , 32>( tile, s_ ## B); \n"
"\n"
"     reduceBucket( 0 )\n"
"     reduceBucket( 1 )\n"
"     reduceBucket( 2 )\n"
"     reduceBucket( 3 )\n"
"     reduceBucket( 4 )\n"
"     reduceBucket( 5 )\n"
"     reduceBucket( 6 )\n"
"     reduceBucket( 7 )\n"
"     reduceBucket( 8 )\n"
"     reduceBucket( 9 )\n"
"     reduceBucket( 10 )\n"
"     reduceBucket( 11 )\n"
"\n"
"\n"
"        //printf(\"summing blk,tid=%d,%d\\n\",blockIdx.x,threadIdx.x);\n"
"       if (threadIdx.x ==0 )\n"
"       {\n"
"          atomicAdd( (unsigned long long int*)&(offset[0]), s_0);\n"
"          atomicAdd( (unsigned long long int*)&(offset[1]), s_1);\n"
"          atomicAdd( (unsigned long long int*)&(offset[2]), s_2);\n"
"          atomicAdd( (unsigned long long int*)&(offset[3]), s_3);\n"
"          atomicAdd( (unsigned long long int*)&(offset[4]), s_4);\n"
"          atomicAdd( (unsigned long long int*)&(offset[5]), s_5);\n"
"          atomicAdd( (unsigned long long int*)&(offset[6]), s_6);\n"
"          atomicAdd( (unsigned long long int*)&(offset[7]), s_7);\n"
"          atomicAdd( (unsigned long long int*)&(offset[8]), s_8);\n"
"          atomicAdd( (unsigned long long int*)&(offset[9]), s_9);\n"
"          atomicAdd( (unsigned long long int*)&(offset[10]),s_10);\n"
"          atomicAdd( (unsigned long long int*)&(offset[11]),s_11);\n"
"       }\n"
"       __syncthreads();\n"
"       \n"
"\n"
"\n"
"    if( gridDim.x >= 12)\n"
"    {\n"
"        // Cumulative sum across blocks for each bucket \n"
"        if (blockIdx.x <12)\n"
"           blockBucketExclusiveSum( blockIdx.x, blockbucket, nblocks ) ;\n"
"    }\n"
"    else\n"
"    {\n"
"        if (blockIdx.x == 0)\n"
"        {\n"
"           blockBucketExclusiveSum( 0, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 1, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 2, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 3, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 4, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 5, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 6, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 7, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 8, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 9, blockbucket, nblocks ) ;\n"
"           blockBucketExclusiveSum( 10, blockbucket, nblocks) ;\n"
"           blockBucketExclusiveSum( 11, blockbucket, nblocks) ;\n"
"        }\n"
"    }\n"
"    \n"
"    \n"
"    \n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // last threadblock saves the cumsum of the 12 global buckets\n"
"    //--------------------------------------------------------------------------\n"
"    /* do on cpu\n"
"    if (blockIdx.x == 0) // gridDim.x - 1)\n"
"    {\n"
"\n"
"        // the last threadblock: compute all 12 global bucket sizes, and its\n"
"        // cumulative sum\n"
"        if (threadIdx.x == 0)\n"
"        {\n"
"            // the work in this last threadblock is single-threaded\n"
"            uint64_t s = 0;\n"
"            for (int bucket = 0 ; bucket < 12 ; bucket++)\n"
"            {\n"
"                // write the global cumsum of all buckets to the final global\n"
"                // bucketp.  bucketp [bucket] is the starting position in\n"
"                // the bucket.\n"
"                bucketp [bucket] = s ;\n"
"                \n"
"                // bucket_size is the total # of entries in this bucket, for\n"
"                // all threadblocks.  It has nearly been computed already,\n"
"                // since offset [bucket] = sum (blockbucket (bucket,0:blockDim.x-1)).\n"
"                // All that is left is to add the counts for the last threadblock.`\n"
"                //int64_t global_bucket_size = offset [bucket];   \n"
"                     // + blockbucket [bucket * gridDim.x + blockIdx.x] ;\n"
"\n"
"                //printf(\"bucketp[%d]= %ld\\n\",bucket, s);\n"
"                // s is a cumulative sum of the global bucket sizes\n"
"                s += offset[bucket]; // global_bucket_size ;\n"
"            }\n"
"            // The kth global bucket (for k = 0 to 11) appears in:\n"
"            // bucket [bucketp [k]... bucketp [k+1]-1],\n"
"            // so the end of the last bucket needs bucketp [12].\n"
"            bucketp [12] = (int64_t)s;\n"
"                //printf(\"bucketp[12]= %ld\\n\", s);\n"
"            // all entries in C now appear in the buckets.\n"
"            // ASSERT (s == cnz) ;\n"
"        }\n"
"        __syncthreads ( ) ;\n"
"    }\n"
"    */\n"
"\n"
"} // phase2 \n"
"\n"
"\n"
"__global__ \n"
"void GB_AxB_dot3_phase2end\n"
"(\n"
"    // input, not modified:\n"
"    int64_t *__restrict__ nanobuckets,    // array of size 12-blockDim.x-by-nblocks\n"
"    const int64_t *__restrict__ blockbucket,    // global bucket count, of size 12*nblocks\n"
"    // output:\n"
"    const int64_t *__restrict__ bucketp,        // global bucket cumsum, of size 13 \n"
"    int64_t *__restrict__ bucket,         // global buckets, of size cnz (== mnz)\n"
"    const int64_t *__restrict__ offset,        // global offsets, for each bucket\n"
"    // inputs, not modified:\n"
"    const GrB_Matrix C,            // output matrix\n"
"    const int64_t cnz        // number of entries in C and M \n"
")\n"
"{\n"
"\n"
"\n"
"    int64_t *__restrict__ Ci = C->i ;       // for zombies, or bucket assignment\n"
"    int64_t *__restrict__ Mp = C->p ;       // for offset calculations \n"
"    int64_t mnvec = C->nvec;\n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // load and shift the nanobuckets for this thread block\n"
"    //--------------------------------------------------------------------------\n"
"\n"
"    // The taskbucket for this threadblock is an array of size\n"
"    // 12-by-blockDim.x, held by row.  It forms a 2D array within the 3D\n"
"    // nanobuckets array.\n"
"    int64_t *__restrict__ taskbucket = nanobuckets + blockIdx.x * (12 * blockDim.x) ;\n"
"\n"
"    //printf(\"block%d thd%d blockbucket= %ld\\n\", blockIdx.x, threadIdx.x, \n"
"    //                                           blockbucket[blockIdx.x*gridDim.x+blockIdx.x]);\n"
"\n"
"    // Each thread in this threadblock owns one column of this taskbucket, for\n"
"    // its set of 12 nanobuckets.  The nanobuckets are a column of length 12,\n"
"    // with stride equal to blockDim.x.\n"
"    int64_t *__restrict__ nanobucket = taskbucket + threadIdx.x;\n"
"\n"
"    // Each thread loads its 12 nanobucket values into registers.\n"
"    #define LOAD_NANOBUCKET(bucket)                     \\\n"
"        int64_t my_bucket_ ## bucket =                  \\\n"
"            nanobucket [bucket * blockDim.x]            \\\n"
"         + blockbucket [bucket * gridDim.x + blockIdx.x]\\\n"
"         + bucketp [bucket] ;                          \n"
"\n"
"    LOAD_NANOBUCKET (0) ;\n"
"    LOAD_NANOBUCKET (1) ;\n"
"    LOAD_NANOBUCKET (2) ;\n"
"    LOAD_NANOBUCKET (3) ;\n"
"    LOAD_NANOBUCKET (4) ;\n"
"    LOAD_NANOBUCKET (5) ;\n"
"    LOAD_NANOBUCKET (6) ;\n"
"    LOAD_NANOBUCKET (7) ;\n"
"    LOAD_NANOBUCKET (8) ;\n"
"    LOAD_NANOBUCKET (9) ;\n"
"    LOAD_NANOBUCKET (10) ;\n"
"    LOAD_NANOBUCKET (11) ;\n"
"\n"
"    // Now each thread has an index into the global set of 12 buckets,\n"
"    // held in bucket, of where to place its own entries.\n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // construct the global buckets\n"
"    //--------------------------------------------------------------------------\n"
"\n"
"    // The slice for task blockIdx.x contains entries pfirst:plast-1 of M and\n"
"    // C, which is the part of C operated on by this threadblock.\n"
"    int64_t pfirst, plast ;\n"
"\n"
"    /*\n"
"    for ( int tid_global = threadIdx.x + blockIdx.x * blockDim.x ;\n"
"              tid_global < (mnvec+7)/8 ;\n"
"              tid_global += blockDim.x * gridDim.x)\n"
"    */\n"
"    int chunk_max= (cnz + chunksize -1)/chunksize;\n"
"    for ( int chunk = blockIdx.x;\n"
"              chunk < chunk_max;\n"
"              chunk += gridDim.x ) \n"
"    {\n"
"\n"
"    //GB_PARTITION (pfirst, plast, cnz, tid_global, (mnvec+7)/8 ) ;\n"
"      pfirst = chunksize * chunk ; \n"
"      plast  = GB_IMIN( chunksize * (chunk+1), cnz ) ;\n"
"\n"
"      int chunk_end;\n"
"      if ( cnz > chunksize) chunk_end = GB_IMIN(  chunksize, \n"
"                                                  cnz - chunksize*(chunk) ); \n"
"      else chunk_end = cnz;\n"
"\n"
"    // find the first vector of the slice for task blockIdx.x: the\n"
"    // vector that owns the entry Ai [pfirst] and Ax [pfirst].\n"
"    //kfirst = GB_search_for_vector_device (pfirst, Mp, 0, mnvec) ;\n"
"\n"
"    // find the last vector of the slice for task blockIdx.x: the\n"
"    // vector that owns the entry Ai [plast-1] and Ax [plast-1].\n"
"    //klast = GB_search_for_vector_device (plast-1, Mp, kfirst, mnvec) ;\n"
"    \n"
"\n"
"    for ( int p = pfirst + threadIdx.x;\n"
"              p < pfirst + chunk_end;\n"
"              p += blockDim.x )\n"
"    {\n"
"        // get the entry C(i,j), and extract its bucket.  Then\n"
"        // place the entry C(i,j) in the global bucket it belongs to.\n"
"\n"
"        // TODO: these writes to global are not coalesced.  Instead: each\n"
"        // threadblock could buffer its writes to 12 buffers and when the\n"
"        // buffers are full they can be written to global.\n"
"        int ibucket = Ci[p] & 0xF;\n"
"        //printf(\" thd: %d p,Ci[p] = %ld,%ld,%d\\n\", threadIdx.x, p, Ci[p], irow );\n"
"        switch (ibucket)\n"
"        {\n"
"            case  0: bucket [my_bucket_0++ ] = p ; Ci[p] = Ci[p] >>4; break ; //unshift zombies\n"
"            case  1: bucket [my_bucket_1++ ] = p ; break ;\n"
"            case  2: bucket [my_bucket_2++ ] = p ; break ;\n"
"            case  3: bucket [my_bucket_3++ ] = p ; break ;\n"
"            case  4: bucket [my_bucket_4++ ] = p ; break ;\n"
"            case  5: bucket [my_bucket_5++ ] = p ; break ;\n"
"            case  6: bucket [my_bucket_6++ ] = p ; break ;\n"
"            case  7: bucket [my_bucket_7++ ] = p ; break ;\n"
"            case  8: bucket [my_bucket_8++ ] = p ; break ;\n"
"            case  9: bucket [my_bucket_9++ ] = p ; break ;\n"
"            case 10: bucket [my_bucket_10++] = p ; break ;\n"
"            case 11: bucket [my_bucket_11++] = p ; break ;\n"
"            default: break; \n"
"        }\n"
"        \n"
"    }\n"
"    //__syncthreads();\n"
"  } \n"
"    \n"
"}\n"
"\n"
;
