const char* const templates_GB_jit_AxB_dot3_phase1_cu = "templates/GB_jit_AxB_dot3_phase1.cu\n"
"//------------------------------------------------------------------------------\n"
"// templates/GB_AxB_cuda_dot3_phase1: symbolic load balancing and data partition\n"
"// to assign work to different 'buckets' for later compute\n"
"//------------------------------------------------------------------------------\n"
"\n"
"//  This kernel scans the non-zero pattern in A and B, takes into account the\n"
"//  mask and computes total work required to form C. Then it classifies each\n"
"//  dot product into a set of buckets for efficient compute. \n"
"\n"
"#define GB_KERNEL\n"
"#include <limits>\n"
"#include <cstdint>\n"
"#include \"matrix.h\"\n"
"#include \"GB_cuda_buckets.h\"\n"
"#include \"local_cub/block/block_scan.cuh\"\n"
"\n"
"//------------------------------------------------------------------------------\n"
"// GB_bucket_assignment\n"
"//------------------------------------------------------------------------------\n"
"\n"
"// assign the dot product C(i,j) = A(:,i)'*B(:,j) to a specific bucket\n"
"__device__ static inline GB_bucket_code GB_bucket_assignment\n"
"(\n"
"    int64_t ainz,       // # of entries A(:,i), always > 0\n"
"    int64_t bjnz,       // # of entries B(:,j), always > 0\n"
"    int64_t vlen        // vector length of A(:,i) and B(:,j)\n"
")\n"
"{\n"
"\n"
"    int b = 0 ; // no bucket assigned yet\n"
"\n"
"    // GB_BUCKET (condition,bucket) :  assigns an entry to a bucket,\n"
"    // if the condition holds, but without using any if statements.\n"
"    // An entry is assigned once and not reassigned.\n"
"\n"
"    // If the bucket b has not assigned, it is b = 0.  The GB_BUCKET function\n"
"    // tests this case, and if the condition is also true, the expression\n"
"    // (b==0) * condition * (bucket+1) becomes equal to bucket+1.  This\n"
"    // value is added to b, which is zero, so the final result is that b\n"
"    // is set to bucket+1.\n"
"\n"
"    // If the bucket b has been assigned already, we have b > 0.  Thus,\n"
"    // the expression ((b==0) * condition * (bucket+1)) becomes zero.\n"
"    // When added to b, the result is that b doesn't change, so the bucket\n"
"    // assignment b is unmodified.\n"
"\n"
"    #define GB_BUCKET(condition,bucket) \\\n"
"        b = (((b == 0) * (condition)) * (bucket+1)) + b ;\n"
"\n"
"//  if (ia_last < ib_first || ib_last < ia_first)\n"
"    { \n"
"\n"
"        //----------------------------------------------------------------------\n"
"        // pattern of A(:,i) and B(:,j) do not overlap\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        // The patterns of A(:,i) and B(:,j) are always sorted.  If the last\n"
"        // entry in A(:,i) comes before the first entry in B(:,j), or visa\n"
"        // versa, then there is no work to do since C(i,j) must be a zombie.\n"
"\n"
"        // GB_BUCKET (ia_last < ib_first || ib_last < ia_first, GB_BUCKET_ZOMBIE);\n"
"\n"
"    }\n"
"//  else if (bjnz == vlen && ainz == vlen && vlen > 256)\n"
"    {\n"
"\n"
"        //----------------------------------------------------------------------\n"
"        // both A(:,i) and B(:,j) are dense\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        // No search of A(:,i) or B(:,j) is needed.  Total work is O(vlen).\n"
"        // The intersection is non-empty, so C(i,j) cannot be a zombie.\n"
"\n"
"        // CUDA kernel: templates/GB_jit_AxB_dot3_phase3_dndn.cu.jit\n"
"\n"
"        GB_BUCKET (bjnz == vlen && ainz == vlen && vlen > 256, GB_BUCKET_DNDN) ;\n"
"\n"
"    }\n"
"//  else if (ainz == vlen)\n"
"    {\n"
" \n"
"        //----------------------------------------------------------------------\n"
"        // A(:,i) is dense and B(:,j) is sparse\n"
"        //----------------------------------------------------------------------\n"
" \n"
"        // No search of A(:,i) is needed.  Total work is O(bjnz), via a linear\n"
"        // time scan of B(:,j).  Since A(:,i) is dense and B(:,j) is non-empty,\n"
"        // the intersection is non-empty, so C(i,j) cannot be a zombie.\n"
"\n"
"        // CUDA kernel: templates/GB_jit_AxB_dot3_phase3_spdn.cu.jit\n"
"        // Two buckets are used, depending on bjnz.\n"
"        GB_BUCKET (ainz == vlen && bjnz <  256, GB_BUCKET_DNVS) ;\n"
"        GB_BUCKET (ainz == vlen && bjnz >= 256, GB_BUCKET_DNSP) ;\n"
" \n"
"    }\n"
"//  else if (bjnz == vlen)\n"
"    {\n"
"\n"
"        //----------------------------------------------------------------------\n"
"        // A(:,i) is sparse and B(:,j) is dense\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        // No search of B(:,j) is needed.  Total work is O(ainz), via a linear\n"
"        // time scan of A(:,i).  Since B(:,j) is dense and A(:,i) is non-empty,\n"
"        // the intersection is non-empty, so C(i,j) cannot be a zombie.\n"
"\n"
"        // CUDA kernel: templates/GB_jit_AxB_dot3_phase3_spdn.cu.jit\n"
"        // Two buckets are used, depending on ainz.\n"
"        GB_BUCKET (bjnz == vlen && ainz <  256, GB_BUCKET_VSDN) ;\n"
"        GB_BUCKET (bjnz == vlen && ainz >= 256, GB_BUCKET_SPDN) ;\n"
"\n"
"    }\n"
"//  else if ((ainz > 32 * bjnz && bjnz < 256)\n"
"//        || (bjnz > 32 * ainz && ainz < 256))\n"
"    {\n"
"\n"
"        //----------------------------------------------------------------------\n"
"        // A(:,i) is very sparse compared to B(:,j), or visa versa\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        // Since B(:,j) is small, and much smaller than A(:,i), the efficient\n"
"        // way to compute C(i,j) is a linear scan of B(:,j).  For each B(k,j),\n"
"        // a binary search for the index A(k,i) is done.  The expected work to\n"
"        // compute C(i,j) is thus O(bjnz * log2 (ainz)).  If A(:,i) is very\n"
"        // sparse compared to B(:,j), the opposite is done inside the kernel.\n"
"\n"
"        // CUDA kernel: templates/GB_jit_AxB_dot3_phase3_vssp.cu.jit\n"
"\n"
"        GB_BUCKET ((ainz > 32 * bjnz && bjnz < 256)\n"
"                || (bjnz > 32 * ainz && ainz < 256), GB_BUCKET_VSSP) ;\n"
"\n"
"    }\n"
"//  else if (ainz + bjnz <= 4)\n"
"    {\n"
"\n"
"        //----------------------------------------------------------------------\n"
"        // both A(:,i) and B(:,j) are very tiny (total size 4 or less)\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        // CUDA kernel: templates/GB_jit_AxB_dot3_phase3_vsvs.cu.jit\n"
"        //GB_BUCKET (ainz + bjnz <= 4, GB_BUCKET_VSVS_4) ;\n"
"\n"
"    }\n"
"//  else if (ainz + bjnz <= 16)\n"
"    {\n"
"\n"
"        //----------------------------------------------------------------------\n"
"        // both A(:,i) and B(:,j) are tiny (total size 16 or less)\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        // CUDA kernel: templates/GB_jit_AxB_dot3_phase3_vsvs.cu.jit\n"
"        //GB_BUCKET (ainz + bjnz <= 16, GB_BUCKET_VSVS_16) ;\n"
"\n"
"    }\n"
"//  else if (ainz + bjnz <= 64)\n"
"    {\n"
"\n"
"        //----------------------------------------------------------------------\n"
"        // both A(:,i) and B(:,j) are small (total size 64 or less)\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        // CUDA kernel: templates/GB_jit_AxB_dot3_phase3_vsvs.cu.jit\n"
"        //GB_BUCKET (ainz + bjnz <= 64, GB_BUCKET_VSVS_64) ;\n"
"\n"
"    }\n"
"//  else if (ainz + bjnz <= 256)\n"
"    {\n"
"\n"
"        //----------------------------------------------------------------------\n"
"        // both A(:,i) and B(:,j) are modest in size (total size 256 or less)\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        // CUDA kernel: templates/GB_jit_AxB_dot3_phase3_vsvs.cu.jit\n"
"        GB_BUCKET (ainz + bjnz <= 256, GB_BUCKET_VSVS_256) ;\n"
"\n"
"    }\n"
"//  else\n"
"    {\n"
"\n"
"        //----------------------------------------------------------------------\n"
"        // default: use the merge-path method\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        // CUDA kernel: templates/GB_jit_AxB_dot3_phase3_mp.cu.jit\n"
"        GB_BUCKET (true, GB_BUCKET_MERGEPATH) ;\n"
"    }\n"
"\n"
"    // subtract one to undo the \"bucket+1\" assignment in the\n"
"    // GB_BUCKET macro assignment expression.\n"
"    return (GB_bucket_code) (b-1) ;\n"
"}\n"
"\n"
"\n"
"//--------------------------------------------------------------------------\n"
"// GB_AxB_cuda_dot3_phase1: build nanobuckets, hunt for pre-zombies\n"
"//--------------------------------------------------------------------------\n"
"\n"
"// GB_AxB_cuda_dot3_phase1 is a CUDA kernel that scans all entries in C and\n"
"// assigns them to each of the 12 buckets.  The output is a 12-by-blockDim array of\n"
"// bucket counts, per threadblock (the nanobucket array).  Each of the blockDim.x \n"
"// threads has its own set of 12 bucket counts.  Each threadblock in this\n"
"// kernel then computes the first part of the cumulative sum of the\n"
"// nanobuckets, and writes it to global memory.\n"
"\n"
"// The kernel also computes Ci, of size nnz(C), which contains the\n"
"// zombie assignment or bucket assignment for non-zombies in C.\n"
"\n"
"template<typename Type_M> \n"
"__global__ void GB_AxB_cuda_dot3_phase1\n"
"(\n"
"    // outputs, preallocated in global memory:\n"
"    int64_t *nanobuckets,       // array of size 12-blockDim.x-by-gridDim.x\n"
"    int64_t *blockbucket,       // bucket counts, of size 12-by-gridDim.x\n"
"    // input/output:\n"
"    GrB_Matrix C,               // final output matrix\n"
"    // inputs, not modified:\n"
"    const GrB_Matrix M,         // mask matrix\n"
"    const GrB_Matrix A,         // input matrix\n"
"    const GrB_Matrix B          // input matrix\n"
")\n"
"{\n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // get C, M, A, and B\n"
"    //--------------------------------------------------------------------------\n"
"    \n"
"    const int64_t *__restrict__ Mh = M->h ;\n"
"    const int64_t *__restrict__ Mp = M->p ;\n"
"    const int64_t *__restrict__ Mi = M->i ;\n"
"    const Type_M *__restrict__ Mx = (Type_M*)M->x ;    // not accessed if M is structural\n"
"    const int64_t mnvec = M->nvec ;\n"
"    const int64_t mnz =  GB_nnz(M) ;\n"
"    const bool M_is_hyper = M->is_hyper ;\n"
"\n"
"    const int64_t *__restrict__ Ah = A->h ;\n"
"    const int64_t *__restrict__ Ap = A->p ;\n"
"    const int64_t *__restrict__ Ai = A->i ;\n"
"    const int64_t avlen = A->vlen ;\n"
"    const int64_t anz = GB_nnz(A) ;\n"
"    const bool A_is_hyper = A->is_hyper ;\n"
"\n"
"    const int64_t *__restrict__ Bh = B->h ;\n"
"    const int64_t *__restrict__ Bp = B->p ;\n"
"    const int64_t *__restrict__ Bi = B->i ;\n"
"    const int64_t bvlen = B->vlen ;\n"
"    const int64_t bnz = GB_nnz(B);\n"
"    const bool B_is_hyper = B->is_hyper ;\n"
"\n"
"    // int64_t *restrict Cp = C->p ;    // copy of Mp\n"
"    // int64_t *restrict Ch = C->h ;    // copy of Mh\n"
"    int64_t *__restrict__ Ci = C->i ;       // for zombies, or bucket assignment\n"
"\n"
"    // Ci [p] for an entry C(i,j) contains either GB_FLIP(i) if C(i,j) is a\n"
"    // zombie, or (k << 4) + bucket otherwise, where C(:,j) is the kth vector\n"
"    // of C (j = Ch [k] if hypersparse or j = k if standard sparse), and\n"
"    // where bucket is the bucket assignment for C(i,j). \n"
"    // bucket can be recovered from Ci by bucket = Ci & 0xF\n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // clear the bucket counters\n"
"    //--------------------------------------------------------------------------\n"
"\n"
"    //ASSERT (mnz > 0) ;\n"
"    //ASSERT (gridDim.x <= mnz) ;\n"
"\n"
"    // each thread uses 12 bucket counters, held in register\n"
"    int64_t my_bucket_0  = 0 ;\n"
"    int64_t my_bucket_1  = 0 ;\n"
"    int64_t my_bucket_2  = 0 ;\n"
"    int64_t my_bucket_3  = 0 ;\n"
"    int64_t my_bucket_4  = 0 ;\n"
"    int64_t my_bucket_5  = 0 ;\n"
"    int64_t my_bucket_6  = 0 ;\n"
"    int64_t my_bucket_7  = 0 ;\n"
"    int64_t my_bucket_8  = 0 ;\n"
"    int64_t my_bucket_9  = 0 ;\n"
"    int64_t my_bucket_10 = 0 ;\n"
"    int64_t my_bucket_11 = 0 ;\n"
"\n"
"    // Registers cannot be indexed (!) so this macro is used instead.\n"
"    // The bucket registers are indexed by the GB_bucket_code enum.\n"
"    #define GB_BUCKET_COUNT(bucket)                 \\\n"
"    {                                               \\\n"
"        switch (bucket)                             \\\n"
"        {                                           \\\n"
"            case  0: my_bucket_0++  ; break ;       \\\n"
"            case  1: my_bucket_1++  ; break ;       \\\n"
"            case  2: my_bucket_2++  ; break ;       \\\n"
"            case  3: my_bucket_3++  ; break ;       \\\n"
"            case  4: my_bucket_4++  ; break ;       \\\n"
"            case  5: my_bucket_5++  ; break ;       \\\n"
"            case  6: my_bucket_6++  ; break ;       \\\n"
"            case  7: my_bucket_7++  ; break ;       \\\n"
"            case  8: my_bucket_8++  ; break ;       \\\n"
"            case  9: my_bucket_9++  ; break ;       \\\n"
"            case 10: my_bucket_10++ ; break ;       \\\n"
"            case 11: my_bucket_11++ ; break ;       \\\n"
"        }                                           \\\n"
"    }\n"
"     /*\n"
"    if(threadIdx.x==0 ) {\n"
"       printf(\" in phase1 kernel, mnz,anz,bnz= %ld,%ld,%ld\\n\",mnz,anz,bnz); \n"
"    }\n"
"    __syncthreads();\n"
"     */\n"
"     #define pointerchunk 256\n"
"\n"
"     __shared__ int64_t Mps[pointerchunk];\n"
"     __shared__ int64_t ks [chunksize];\n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // compute the task descriptor\n"
"    //--------------------------------------------------------------------------\n"
"\n"
"    // all threads in this block will compute the same values for these:\n"
"    int32_t pfirst, plast, kfirst, klast ;\n"
"    /*\n"
"    for ( int tid_global = threadIdx.x + blockIdx.x * blockDim.x ; \n"
"              tid_global < (mnvec+ 7)/8 ;\n"
"              tid_global += blockDim.x*gridDim.x) \n"
"              */\n"
"    int chunk_max= (mnz + chunksize -1)/chunksize;\n"
"    for ( int chunk = blockIdx.x;\n"
"              chunk < chunk_max;\n"
"              chunk += gridDim.x ) \n"
"    {\n"
"\n"
"      // The slice for each task contains entries pfirst:plast-1 of M and C.\n"
"      //GB_PARTITION (pfirst, plast, mnz, chunk, (mnz+1023)/1024 ) ;\n"
"      pfirst = chunksize * chunk ; \n"
"      plast  = GB_IMIN( chunksize * (chunk+1), mnz ) ;\n"
"\n"
"      int chunk_end;\n"
"      if ( mnz > chunksize) chunk_end = GB_IMIN(  chunksize, \n"
"                                                  mnz - chunksize*(chunk) ) ; \n"
"      else chunk_end = mnz;\n"
"\n"
"      // find the first vector of the slice for task tid_global: the\n"
"      // vector that owns the entry Ai [pfirst] and Ax [pfirst].\n"
"      kfirst = GB_search_for_vector_device (pfirst, Mp, 0, mnvec) -1 ;\n"
"      //if( pfirst ==0) kfirst = 0;\n"
"\n"
"      // find the last vector of the slice for task blockIdx.x: the\n"
"      // vector that owns the entry Ai [plast-1] and Ax [plast-1].\n"
"      klast = GB_search_for_vector_device (plast-1, Mp, kfirst, mnvec) ;\n"
"\n"
"      int k_end = GB_IMIN(  pointerchunk ,  klast - kfirst +2 ) ;\n"
"       /* \n"
"      if( threadIdx.x ==0) \n"
"      {\n"
"         printf(\"chunk%d pfirst,plast,ch_end =%d,%d,%d kfirst,klast,kend = %d,%d,%d\\n\",\n"
"                 chunk, pfirst, plast, chunk_end, kfirst, klast, k_end ) ;\n"
"      }\n"
"      __syncthreads();\n"
"      */\n"
"      \n"
"     \n"
"      // load pointer values for this chunk\n"
"      for ( int i = threadIdx.x; i< k_end; i+= blockDim.x)\n"
"      {\n"
"          Mps[i] = Mp[i + kfirst];\n"
"      }\n"
"      __syncthreads();\n"
"\n"
"      // search for k values for each entry\n"
"      float slope = (float)(mnvec)/(float)(mnz* chunksize) ;\n"
"      for ( int i =  threadIdx.x; i< chunk_end; i+= blockDim.x)\n"
"      {   \n"
"          ks[i] = kfirst + slope*( float )(i);\n"
"          while ( Mps[ ks[i] - kfirst + 1 ] <= (i+pfirst) )\n"
"             ks[i]++;\n"
"          while ( Mps[ ks[i] - kfirst     ] >  (i+pfirst) )\n"
"             ks[i]--;\n"
"      }\n"
"      __syncthreads();\n"
"\n"
"\n"
"    //ASSERT (0 <= kfirst && kfirst <= klast && klast < mnvec) ;\n"
"    /*\n"
"    if (threadIdx.x ==0 ) {\n"
"       printf (\"threadblock %d  after ksearch pfirst %ld plast %ld kfirst %ld klast %ld\\n\",\n"
"                blockIdx.x, pfirst, plast, kfirst, klast) ;\n"
"    }\n"
"    __syncthreads();\n"
"    */\n"
"    \n"
"    \n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // assign entries in C(i,j) to the buckets\n"
"    //--------------------------------------------------------------------------\n"
"\n"
"    // if B is hypersparse, bpleft ... TODO describe\n"
"    // int64_t bpleft = 0 ;\n"
"    \n"
"        //----------------------------------------------------------------------\n"
"        // no binary search variant\n"
"        //----------------------------------------------------------------------\n"
"\n"
"        //printf (\"no binary search\\n\") ;\n"
"\n"
"        //int32_t pM_start, pM_end ;\n"
"        //for (int64_t pM = pfirst + threadIdx.x ; pM < plast ; pM += blockDim.x)\n"
"        int32_t i,j;\n"
"        int32_t k = kfirst ;\n"
"            \n"
"        //for (int64_t pM = pfirst; pM < plast; pM++ ) \n"
"        for ( int pM = pfirst + threadIdx.x;\n"
"                  pM < pfirst + chunk_end;\n"
"                  pM += blockDim.x )\n"
"        {\n"
"            GB_bucket_code bucket = GB_BUCKET_ZOMBIE ;\n"
"            k = ks[ pM - pfirst ] ;\n"
"            //k += ( pM == Mp[k+1] ) ;\n"
"            //printf (\"tid%d  k %ld pM %ld\\n\", tid_global, k, pM;\n"
"            i = Mi [ pM ] ;\n"
"\n"
"            if ( MX ( pM ) )\n"
"            { \n"
"\n"
"            // do a binary search for k (and j) that has this entry M(i,j)\n"
"            //k = GB_search_for_vector_device (pM, Mp, k, klast) ;\n"
"\n"
"// HACK\n"
"j = k ;\n"
"//          int64_t j = (Mh == NULL) ? k : Mh [k] ;\n"
"\n"
"            //--------------------------------------------------------------\n"
"            // get B(:,j)\n"
"            //--------------------------------------------------------------\n"
"\n"
"            int64_t pB, pB_end ;\n"
"// HACK: for sparse only, not hypersparse\n"
"pB = Bp [j] ;\n"
"pB_end = Bp [j+1] ;\n"
"//              GB_lookup_device (B_is_hyper, Bh, Bp, &bpleft, bnvec-1, j,\n"
"//                  &pB, &pB_end) ;\n"
"                int64_t bjnz = pB_end - pB ;\n"
"                if (bjnz > 0)\n"
"                {\n"
"                 //   int64_t ib_first = Bi [pB] ;\n"
"                 //   int64_t ib_last  = Bi [pB_end-1] ;\n"
"\n"
"                    //----------------------------------------------------------\n"
"                    // get A(:,i)\n"
"                    //----------------------------------------------------------\n"
"\n"
"                    int64_t pA, pA_end ;\n"
"                    //int64_t apleft = 0 ;\n"
"// HACK: for sparse only, not hypersparse\n"
"pA = Ap [i] ;\n"
"pA_end = Ap [i+1] ;\n"
"//                  GB_lookup_device (A_is_hyper, Ah, Ap, &apleft, anvec-1, i,\n"
"//                      &pA, &pA_end) ;\n"
"                    int64_t ainz = pA_end - pA ;\n"
"                    if (ainz > 0)\n"
"                    {\n"
"                     //   int64_t ia_first = Ai [pA] ;\n"
"                     //   int64_t ia_last  = Ai [pA_end-1] ;\n"
"\n"
"                        //------------------------------------------------------\n"
"                        // determine the bucket for C(i,j)\n"
"                        //------------------------------------------------------\n"
"\n"
"                        //bucket = GB_BUCKET_MERGEPATH ;\n"
"                         bucket= GB_bucket_assignment ( ainz, bjnz, bvlen) ;\n"
"                    }\n"
"                }\n"
"            }\n"
"\n"
"            if (bucket == GB_BUCKET_ZOMBIE)\n"
"            {\n"
"                // mark C(i,j) is a zombie\n"
"                //printf (\"tid%d pM=%d %d,%d prezombie\\n\",threadIdx.x,pM,i,j) ;\n"
"                Ci [pM] = GB_FLIP (i) << 4 ;\n"
"                // GB_BUCKET_COUNT (GB_BUCKET_ZOMBIE) ;\n"
"                my_bucket_0++ ; //0 is the zombie bucket\n"
"            }\n"
"            else\n"
"            {\n"
"                // place C(i,j) in its bucket\n"
"                Ci [pM] = (k << 4) + bucket ;\n"
"                GB_BUCKET_COUNT (bucket) ;\n"
"                //printf (\"tid%d pM=%d %d,%d b=%d\\n\",threadIdx.x, pM, i,j, (int)bucket) ;\n"
"            }\n"
"         }\n"
"            \n"
"        \n"
"    \n"
"    }\n"
"    __syncthreads();\n"
"\n"
"    //--------------------------------------------------------------------------\n"
"    // cumulative sum of each bucket\n"
"    //--------------------------------------------------------------------------\n"
"\n"
"    typedef cub::BlockScan<int64_t, 32, cub::BLOCK_SCAN_WARP_SCANS> BlockCumSum; \n"
"    __shared__ typename BlockCumSum::TempStorage temp_storage;\n"
"\n"
"    // The taskbucket for this thread block is an array of size\n"
"    // 12-by-blockDim.x, held by row.  Each thread owns one column of this\n"
"    // taskbucket, the nanobucket.  The nanobucket is a column of length 12,\n"
"    // with stride equal to blockDim.x.\n"
"    int64_t *nanobucket =\n"
"        nanobuckets + blockIdx.x * (12 * blockDim.x) + threadIdx.x ;\n"
"\n"
"    #define CUMSUM_AND_STORE_NANOBUCKET(bucket)                             \\\n"
"        if( threadIdx.x == blockDim.x-1)                                    \\\n"
"            blockbucket [blockIdx.x + bucket * gridDim.x] =                 \\\n"
"            my_bucket_ ## bucket ;                                          \\\n"
"        BlockCumSum(temp_storage).ExclusiveSum                              \\\n"
"            ( my_bucket_ ## bucket, my_bucket_ ## bucket) ;                 \\\n"
"            __syncthreads();                                                \\\n"
"        nanobucket [bucket * blockDim.x] = my_bucket_ ## bucket ;\n"
"\n"
"    CUMSUM_AND_STORE_NANOBUCKET (0) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (1) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (2) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (3) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (4) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (5) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (6) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (7) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (8) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (9) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (10) ;\n"
"    CUMSUM_AND_STORE_NANOBUCKET (11) ;\n"
"\n"
"    /*    \n"
"    if(threadIdx.x +blockIdx.x*blockDim.x <= mnvec) //blockDim.x -1){ \n"
"    {\n"
"       printf(\"thd %d blk%d nbucket0 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[0]);\n"
"       printf(\"thd %d blk%d nbucket1 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[1*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket2 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[2*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket3 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[3*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket4 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[4*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket5 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[5*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket6 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[6*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket7 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[7*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket8 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[8*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket9 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[9*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket10 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[10*blockDim.x]);\n"
"       printf(\"thd %d blk%d nbucket11 has %ld prev\\n\",threadIdx.x, blockIdx.x, nanobucket[11*blockDim.x]);\n"
"\n"
"    }\n"
"    __syncthreads();\n"
"    */\n"
"        \n"
"\n"
"    // The last thread now has the sum of all nanobuckets, which is then saved\n"
"    // to the global bucket counts.   blockbucket is an array of size\n"
"    // 12-by-gridDim.x, held by row, with one column per thread block.\n"
"    // The last thread saves its result in the column of this thread block.\n"
"    // Note that this write to global memory is not coalesced.\n"
"\n"
"    #define STORE_GLOBAL_BUCKET_COUNT(bucket)                    \\\n"
"        blockbucket [blockIdx.x + bucket * gridDim.x] +=         \\\n"
"            my_bucket_ ## bucket ;\n"
"\n"
"    if (threadIdx.x == blockDim.x - 1 ) \n"
"    {\n"
"        STORE_GLOBAL_BUCKET_COUNT (0) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (1) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (2) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (3) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (4) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (5) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (6) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (7) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (8) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (9) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (10) ;\n"
"        STORE_GLOBAL_BUCKET_COUNT (11) ;\n"
"    }\n"
"    \n"
"    /* \n"
"    if(threadIdx.x == blockDim.x -1){ \n"
"\n"
"       printf(\"block%d bbucket0 has %ld entries\\n\",blockIdx.x, blockbucket[0*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket1 has %ld entries\\n\",blockIdx.x, blockbucket[1*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket2 has %ld entries\\n\",blockIdx.x, blockbucket[2*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket3 has %ld entries\\n\",blockIdx.x, blockbucket[3*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket4 has %ld entries\\n\",blockIdx.x, blockbucket[4*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket5 has %ld entries\\n\",blockIdx.x, blockbucket[5*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket6 has %ld entries\\n\",blockIdx.x, blockbucket[6*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket7 has %ld entries\\n\",blockIdx.x, blockbucket[7*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket8 has %ld entries\\n\",blockIdx.x, blockbucket[8*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket9 has %ld entries\\n\",blockIdx.x, blockbucket[9*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket10 has %ld entries\\n\",blockIdx.x, blockbucket[10*gridDim.x+blockIdx.x]);\n"
"       printf(\"block%d bbucket11 has %ld entries\\n\",blockIdx.x, blockbucket[11*gridDim.x+blockIdx.x]);\n"
"\n"
"    }\n"
"    __syncthreads();\n"
"    */\n"
"    \n"
"}\n"
"\n"
;
